---
title: "Visualisation for Data Mining"
author: "Di Cook & Eun-Kyung Lee"
date: "May 30, 2017 <br> <br> Slides on http://bit.ly/dmvis2017"
output:
  xaringan::moon_reader:
    css: ["default", "myremark.css"]
    self_contained: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r initial, echo = FALSE, cache = FALSE, results = 'hide'}
library(knitr)
options(htmltools.dir.version = FALSE, tibble.width = 60)
opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE, comment = "#>",
  fig.path = 'figure/', cache.path = 'cache/', fig.align = 'center', 
  fig.width = 12, fig.height = 8, fig.show = 'hold', 
  cache = TRUE, external = TRUE, dev = 'CairoPNG', dpi = 300
)
```

## Conceptual framework

- Visual methods for high-dimensional data are well-developed and understood. Toolbox includes:
    + scatterplot matrices
    + parallel coordinate plots
    + interactive tools like brushing and linking
    + dynamic methods like tours
- These methods can also be used to visualise our models, often leading to some surprise that the fitted models may be different from expected.

---

## Questions to answer

- What does the model look like? How does the model change when its parameters change? How do the parameters change when the data is changed?
- How well does the model fit the data? How does the shape of the model compare to the shape of the data? Is the model fitting uniformly good, or good in some regions but poor in other regions? Where might the fit be improved?

---


## Key strategies

- Display the model in data space (m-in-ds), as well as the data in the model space.
- Look at all members of a collection of a model, not just a single one.
- Explore the process of fitting, not just the end result.

---


## Model in the data space

```{r}
library(tidyverse)
library(gridExtra)
df <- data.frame(x=runif(100))
df <- df %>% mutate(y=c(5+3*x[-100]+rnorm(99), 15))
df_lm <- lm(y~x, data=df)
df <- df %>% mutate(res = residuals(df_lm))
p1 <- ggplot(df, aes(x=res)) + geom_histogram(binwidth=0.5) + 
  ggtitle("Data in the model space") +
  xlab("Residuals") + theme(aspect.ratio=1)
coefs <- coefficients(df_lm)
p2 <- ggplot(df, aes(x=x, y=y)) + geom_point() +
  geom_abline(intercept=coefs[1], slope=coefs[2]) +
  ggtitle("Model in the data space") + 
  theme(aspect.ratio=1)
grid.arrange(p1, p2, ncol=2)
```

---


## Model in the data space

```{r}
library(MASS)
library(mvtnorm)
g1 <- rmvnorm(n=120, mean=c(1,2), sigma=matrix(c(4,2,2,3), ncol=2)) 
g2 <- rmvnorm(n=80, mean=c(-1,-3), sigma=matrix(c(6,-3,-3,6), ncol=2))
df <- data.frame(x1=c(g1[,1], g2[,1]), x2=c(g1[,2], g2[,2]), g=c(rep("A", 120), rep("B", 80)))
df_lda <- lda(g~., data=df, prior=c(0.5, 0.5))
df <- df %>% mutate(pred = as.vector(predict(df_lda)$x[,1]))
p1 <- ggplot(df, aes(x=pred)) + 
  geom_histogram(breaks=seq(-3, 5, 0.5)) + 
  geom_vline(xintercept=0, colour="#7570B3") +
  facet_wrap(~g, ncol=1) +
  ggtitle("Data in the model space")
df_grid <- expand.grid(x1=seq(-10,10,0.2), x2=seq(-11,10,0.2)) 
df_grid$pred <- predict(df_lda, df_grid)$class
p2 <- ggplot(df_grid, aes(x=x1, y=x2, colour=pred)) + 
  geom_point(alpha=0.1) +
  geom_point(data=df, aes(x=x1, y=x2, colour=g), 
             shape=4, size=3, inherit.aes = FALSE) +
  scale_colour_brewer("", palette="Dark2") +
  ggtitle("Model in the data space") + 
  theme_bw() + theme(aspect.ratio=1)
grid.arrange(p1, p2, ncol=2)
```

---


class: inverse middle center
## Your Turn

For each of the two previous displays, write down one thing that you learn differently from the two different ways of looking at the model and data.

```{r eval=FALSE}
1. 
D-in-MS: Focus is on residuals, see the outlier clearly
M-in-DS: See that the model fits the middle of the data nicely, outlier may have pulled the line
2. 
D-in-M-S: Predictions are routine, some cases misclassified
M-in-D-S: Variances of the two groups are slightly different
```

---


class: inverse middle center
## Your Turn

This is a typical diagram used to explain how to build a Support Vector Machine. Is it M-in-DS or D-in-MS??

![produced by cyc, creative commons license, available on wikipedia](svm.pdf)

Illustration produced by cyc, creative commons license, available on wikipedia

---


## All members of a collective

- All possible models,  e.g. for a linear regression, we can generate all possible main effects models.
- Same model fit to subsets: neural networks (ensembles of logistic models), random forests (ensembles of trees)

---


## Linear regression

- Model level: model fit statistics. $m$ observations.
- Model-estimate level: coefficient estimates on various scales. $m \times p$ observations.
- Estimate level: summary of estimates over the models. $p$ observations.
- Model-observation level: residuals and influence measures. $m \times n$ observations.
 Observation level: the original data, plus summaries of residual behavior. $n$ observations.

---


## All possible models

```{r results='hide'}
# devtools::install_github("hadley/meifly")
library(meifly)
library(plotly)
library(stringr)

rescale01 <- function(x) (x - min(x, na.rm = TRUE)) / diff(range(x, na.rm = TRUE))

# Fit all linear models to New Haven data --------------------------------------

data(NewHavenResidential, package = "barcode")

y <- scale(NewHavenResidential[,1])
x <- NewHavenResidential[,-c(1,3)]
x$livingArea <- scale(x$livingArea)
x$size <- scale(x$size)
x$bedrms <- scale(x$bedrms)
x$bathrms <- scale(x$bathrms)
models <- fitall(y, x)
# ggobi(models, NewHavenResidential)

# Summary statistics -----------------------------------------------------------

model_sum <- models %>%
  summary() %>%
  tbl_df() %>%
  gather(statistic, val, logL:adjR2) %>%
  group_by(statistic) %>% 
  mutate(stdval = rescale01(val), df = df - 1) %>%
  group_by(statistic, df) %>% mutate(rank = min_rank(desc(val))) %>%
  ungroup()

mnames <- function(m) {
  name <- str_c(names(coefficients(m))[-1], collapse=",")
  return(name)
}
model_r2 <- models %>% 
  plyr::ldply(mnames, .id = "model") %>% 
  dplyr::rename(name=V1) %>%
  right_join(model_sum, by = "model") %>%
  filter(statistic == "R2")

p <- ggplot(model_r2, aes(x=df, y=val, label=name)) +
  geom_point() +
  geom_line(data = filter(model_r2, rank == 1)) +
  #facet_wrap(~statistic, ncol = 5) +
  xlab("Degrees of Freedom") +
  ylab("R2")
```

```{r fig.width=2, fig.height=1.5}
ggplotly(p)
```

---


## Coefficients

```{r}
f_coef <- function(x) {
  coef <- coefficients(x)
  df <- matrix(rep(0, 8), ncol=8)
  colnames(df)=c("(Intercept)","livingArea",
         "size","zoneRM","zoneRS","acTypeNo AC", 
         "bedrms","bathrms")
    data.frame(coef=rep(0, 9))
  df[colnames(df) %in% names(coef)] <- coef
  colnames(df)[1] <- "Intercept"
  return(data.frame(df))
}
model_coef <- models %>% 
  map(f_coef) %>%
  map_df(bind_rows) %>%
  select(-Intercept) %>%
  mutate(id = 1:nrow(.)) %>%
  gather(var, val, -id) #%>%
  #group_by(var) %>% 
  #mutate(stdval = rescale(val))
model_coef$var <- factor(model_coef$var, 
                         levels=unique(model_coef$var)) 

```

```{r fig.width=2, fig.height=1.5}
p <- ggplot(model_coef, aes(x = var, y = val,  
                     key = id, text = id)) + 
  #geom_jitter(width = 0.25, alpha = 0.5) +
    geom_line(aes(group = id), alpha = 0.2) +
    geom_point(alpha = 0.5, size = 0.001) +
    theme_bw() + xlab("") + ylab("Standardised coefficients")
ggplotly(p, tooltip = "text") %>% layout(dragmode = "select")
```

This needs some more work to get brushing going.

---


## Neural networks

$$P(y \in \textrm{class}_j | x_{1}, ..., x_{p}) = k \cdot \phi ( \alpha + \sum_{h=1}^s w_{hj} \phi (\alpha_h + \sum_{i=1}^p w_{ih} x_i ))$$

```{r}
library(nnet)
library(readr)
w <- read_csv("wiggly.csv")[ ,2:4]
w <- transform(w, class = factor(class, labels = c("A", "B")))
sqr <- theme(aspect.ratio = 1)
nolab <- list(scale_x_continuous(""), scale_y_continuous(""))
col_d <- scale_colour_manual(values = c("#377EB8", "#E41A1C"))
fill_c <- scale_fill_gradient2(low = "#E41A1C", mid = "white", high = "#377EB8",
  midpoint = 0.5)
load("wiggly-multi.rdata")
many <- rbind(many2$coef, many3$coef, many4$coef)
many <- plyr::rename(many, c(size = "nodes"))
many$id <- (many$nodes - 2) * 200 + as.integer(many$i)
source("explore.r")
best <- Filter(function(x) accuracy(x) == 1, many4$all)[[1]]
```

```{r}
p1 <- ggplot(subset(best$output,  node == 1), aes(x, y)) +
  geom_raster(aes(fill = pred)) +
  geom_point(aes(shape = class), data = w) +
  sqr +
  fill_c +
  nolab
p2 <- ggplot(best$hidden, aes(x, y)) +
  geom_contour(aes(z = pred, group = node), colour = "grey50", size = 2,
    breaks = 0.5) +
  geom_point(aes(colour = class, shape = class), data = w) +
  sqr + col_d + nolab
grid.arrange(p1, p2, ncol=2)
```

---


## Hidden nodes

```{r}
hidden_fill <- scale_fill_gradient2(low = "#AF8DC3", mid = "#F7F7F7",
  high = "#7FBF7B", midpoint = 0.5, limits = c(0, 1))

ggplot(best$hidden, aes(x, y)) +
  geom_raster(aes(fill = pred)) +
  geom_point(aes(shape = class), data = w) +
  sqr +
  facet_grid(. ~ node) +
  hidden_fill +
  nolab
```

---


class: inverse middle 
## Your Turn

- In the formula for the model, what is $s$ and $p$?
- Sketch the neural network diagram that corresponds to the best model.

---


## Explore the process of fitting

<iframe src="https://player.vimeo.com/video/767832" width="640" height="483" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

---


## Model comparison

```{r}
qual <- unique(many[, c("value", "accuracy", "nodes", "id")])
ggplot(data = qual, aes(x=accuracy, y=value)) + 
  geom_point() +
  xlab("Accuracy") +
  ylab("Value of fitting criterion") + 
  facet_wrap(~ nodes)
```

---


## Boundaries for each model

```{r}
all <- mapply(function(nn, i, j) {
  df <- subset(nn$output, node==1)
  df$id <- i
  df$nodes <- j
  df
}, c(many2$all,many3$all,many4$all), 1:600, 
  rep(2:4, each=200), SIMPLIFY = FALSE)
all <- do.call("rbind", all)

contours <- plyr::ddply(all, plyr::.(nodes, id), 
              function(df) contour(df$x, df$y, df$pred))
contours <- subset(contours, piece == 1)
contours <- merge(contours, qual)
contours$acc <- cut(contours$accuracy, c(0, 0.915, 0.925, 0.98, 1),
  labels = c("Sub-linear", "Linear", "Super-linear", "Excellent"))

ggplot(contours, aes(x, y)) + sqr +
  geom_path(aes(group = id), 
            colour = scales::alpha("grey50", 0.5)) +
  facet_grid(nodes ~ acc) +
  nolab
```

---


## Random forest

- Tree-level: each tree has its own training/test samples
- Variable-level: each variable is ranked by the drop in model performance for each tree
- Observation-level: for each observation we have the distribution of predictions across all trees

We will come back to this later, because the area is very rich.


---

## Visualisation Toolbox

* Grammar of graphics: map variables to graphical elements
* Multivariate data plots: go beyond 2D
* Dimension reduction: get back to 2D 
* Interactive graphics: to explore various aspects quickly, see beyond 2D 

---

## Grammar of graphics

* Tidy data is in a form where variables are provided in columns, and observations in rows. Statistically we have random variables, and a sample from a population. 
* The grammar of graphics (Wilkinson, 1999; Wickham, 2009) provides the mapping of variables to graphical elements:

```{r eval=FALSE, echo=TRUE}
ggplot(df, aes(x=res)) + geom_histogram(binwidth=0.5) 
ggplot(df, aes(x=x, y=y)) + geom_point() +
  geom_abline(intercept=coefs[1], slope=coefs[2]) 
ggplot(df_grid, aes(x=x1, y=x2, colour=pred)) + 
  geom_point(alpha=0.1) +
  geom_point(data=df, aes(x=x1, y=x2, colour=g), 
             shape=4, size=3, inherit.aes = FALSE) +
  scale_colour_brewer("", palette="Dark2")
```

---

## Multivariate plots

* Scatterplot matrix: Matrix layout of all pairs of variables
* Parallel coordinate plot: axes are laid out in parallel, rather than orthogonal, and observations are connected by lines.
* Tours: Movie of all possible low-dimensional projections

---

## Scatterplot matrix

```{r}
library(GGally)
ggpairs(NewHavenResidential, columns=c(1,2,4,5))
```

---

class: inverse middle 

## Your Turn

- What do you learn about the relationship between the four variables?
- What transformations might be recommended?
- Are there other cleaning that you would recommend before modeling?

---

## Parallel coordinate plot

```{r}
library(tourr)
data(olive)
olive$region <- factor(olive$region, levels=1:3, labels=c("South", "Sardinia", "North"))
ggparcoord(olive, columns=3:10, groupColumn=1, order="anyClass")
```

---
## Side-by-side boxplots

```{r}
library(tourr)
data(olive)
olive$region <- factor(olive$region, levels=1:3, labels=c("South", "Sardinia", "North"))
ggparcoord(olive, columns=3:10, groupColumn=1, order="anyClass", boxplot = TRUE, alphaLines = 0.01)
```

---

## Parallel coordinate plot

- Values on each variable are standardised. Other choices of scaling are reasonable.
- Ordering of variables helps perceive structure. Here variables have been ordered by any class difference.
- Structure to be seen:
    + because the coloured lines flow differently, there are large differences between the three classes
    + eicosenoic completely separates south
    + sardinia and north separated in oleic and linoleic
    + linolenic is discrete

---

## Tours

A sequence of $d$-dimensional projections, $XA$, of $p$-dimensional data is shown in an animation or movie.

$$X = \left[ \begin{array}{rrrr} 
         x_{11} & x_{12} & \dots & x_{1p}\\
         x_{21} & x_{22} & \dots & x_{2p}\\
         \vdots & \vdots &  & \vdots \\
         x_{n1} & x_{n2} & \dots & x_{nd}
         \end{array} \right]
         ~~~ A = \left[ \begin{array}{rrrr} 
         a_{11} & a_{12} & \dots & a_{1d}\\
         a_{21} & a_{22} & \dots & a_{2d}\\
         \vdots & \vdots &  & \vdots \\
         a_{p1} & a_{p2} & \dots & a_{pd}
         \end{array} \right]$$
         
- Grand: random walk over all possible projections
- Guided: follows an optimisation path of a function
- Little: interpolates between all marginal views
- Manual: user controls contributions from a variable, rotating a variable into or out of a projection
 
---

## Tours

<iframe src="https://player.vimeo.com/video/137916478" width="277" height="264" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 

<iframe src="https://player.vimeo.com/video/137802511" width="277" height="264" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 

<iframe src="https://player.vimeo.com/video/127615225" width="640" height="203" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 

---

## Tours focus on shapes

- From example 1, you should see 4 clusters. Based on (1) a group separates out in a fleeting moment, (2) following the motion path of the groups, three are easily seen, and eventually largest group breaks into 2. The clusters are primarily spherical.
- In example 2, the two groups are almost always shifted fro each other. Never a gap between the two, and some blue and orange tend to hang with the other group. The clusters are not spherical, and form different shapes in different projections. 
- Shape can be important for making distributional assumptions about data

---
class: inverse middle

## Your Turn

- Make a parallel coordinate plot of the ratcns data in the tourr package
- Read the description of the data `?ratcns`, and decide what scaling choice would be appropriate. Re-make the parallel coordinates accordingly.
- Make a side-by-side boxplot of the ratcns data 
- What do you learn differently from the two displays? Is one better than the other?

```{r eval=FALSE}
data(ratcns)
ggparcoord(ratcns, columns=1:9, alphaLines=0.5, scale="globalminmax")
ggparcoord(ratcns, columns=1:9, boxplot=TRUE,
           alphaLines=0, scale="globalminmax")
```
---

class: inverse middle

## Your Turn

- Make a grand tour of the olive oils data, with points coloured by region
- Are the three regions perfectly separable?
- How many clusters, in addition to the classes, do you see?
- Are there any outliers?

```{r eval=FALSE}
library(RColorBrewer)
pal <- brewer.pal(length(levels(olive$region)), "Dark2")
col <- pal[as.numeric(olive$region)]
quartz()
animate_xy(olive[,3:10], col=col)
```

---

## Dimension reduction

* Principal component analysis (PCA): reduce the dimension by making linear combinations of the original variables, that capture as much of the original variance-covariance as possible. Also called "sphering".
* Multidimensional scaling (MDS): A super-set of PCA that enables creating low-dimensional mapping of data points to best match their distances in the original $p$-space. Often used, when the starting point is purely a distance matrix.  
* Projection pursuit (PP): A super-set of PCA which creates linear combinations of orginal variables, that optimise something other than variance-covariance: e.g. holes, outliers, class differences
* Linear discriminant analysis (LDA) is also a useful method for finding a low-dimensional projection revealing group differences. 

---

## Dimension reduction

```{r}
olive_pca <- prcomp(olive[,3:10], scale=TRUE, retx=TRUE)
olive <- olive %>% mutate(PC1=olive_pca$x[,1], 
                          PC2=olive_pca$x[,2])
p1 <- ggplot(olive, aes(x=PC1, y=PC2, color=region)) +
  geom_point() + theme(aspect.ratio=1) +
  scale_colour_brewer("", palette="Dark2")
olive_mds <- cmdscale(dist(apply(olive[,3:10], 2, scale)), 
                      k=2, x.ret=TRUE)
olive <- olive %>% mutate(MDS1=olive_mds$points[,1],
                          MDS2=olive_mds$points[,2])
p2 <- ggplot(olive, aes(x=MDS1, y=MDS2, color=region)) +
  geom_point() + theme(aspect.ratio=1) +
  scale_colour_brewer("", palette="Dark2")
PP <- data.frame(PP1 = as.matrix(olive[,3:10]) %*% as.matrix(c(0.075, -0.372, 0.113, -0.313, 0.208, 0.102, 0.047, 0.830)), PP2 = as.matrix(olive[,3:10]) %*% as.matrix(c(-0.184, 0.737, -0.299, 0.147, 0.389, 0.233, 0.095, 0.312)))
olive <- olive %>% mutate(PP1=PP$PP1[1:572],
                          PP2=PP$PP2[1:572])
p3 <- ggplot(olive, aes(x=PP1, y=PP2, color=region)) +
  geom_point() + theme(aspect.ratio=1) +
  scale_colour_brewer("", palette="Dark2")
olive_lda <- predict(lda(region~., data=olive[,c(1, 3:10)],
                 prior=c(1,1,1)/3))$x
olive <- olive %>% mutate(LD1=olive_lda[,1],
                          LD2=olive_lda[,2])
p4 <- ggplot(olive, aes(x=LD1, y=LD2, color=region)) +
  geom_point() + theme(aspect.ratio=1) +
  scale_colour_brewer("", palette="Dark2")
grid.arrange(p1, p2, p3, p4, ncol=2)
```

```{r eval=FALSE}
quartz()
animate_xy(olive[, 3:10], tour_path=guided_tour(lda_pp(olive[,1])), 
           sphere = FALSE, col=col)

```

---

## Interactive graphics

I use two current technologies for interactive graphics in R:

- shiny: reactive plots and graphical user interfaces
- plotly: javascript conversion of ggplot2 objects

Example shiny app for a projection pursuit forest: 

[https://natydasilva.shinyapps.io/shinyppforest/](https://natydasilva.shinyapps.io/shinyppforest/)

---

## Supervised Classification

* Checking assumptions
* Examining misclassifications
* Variable importance
* Visualising boundaries
* Exploring ensembles

---

## My notes

- show how to extract SVs
- forest components

## Outline

* The Toolbox: Grammar of graphics, multivariate data plots, dimension reduction, interactive graphics
* Supervised Classification: Checking assumptions, examining misclassifications, variable importance, visualising boundaries, exploring ensembles
* Trees: Exploring splits, oblique trees
* Forests: Variable importance, confusion table, vote matrix, 
proximity matrix, oblique forest shiny app
* Support vector machines: support vectors, boundary
* Neural networks: Components
* High-dimension, small sample size

## Toolbox

* Grammar of graphics
* Multivariate data plots 
* Dimension reduction 
* Interactive graphics


## Checking assumptions

* LDA: equal variance-covariances
* 

## Examining misclassifications

Linking confusion table with data plots

## Variable importance

Ordering par coords by importance

## Visualising boundaries

Predicting grid and viewing with tour

## Exploring ensembles

* Diagnostics
* Pruning

## Trees

## Sources

- Wickham, Cook, Hofmann (2015) Visualizing statistical models: Removing the blindfold, Statistical Analysis and Data Mining, (http://dx.doi.org/10.1002/sam.11271)
- Cook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis: With Examples Using R and GGobi (http://www.ggobi.org/book/)


---

## Acknowledgements

* Slides powered by the R package [xaringan](https://github.com/yihui/xaringan)
  for [remark.js](https://remarkjs.com/) and [R Markdown](https://rmarkdown.rstudio.com)
    + The source files to reproduce the slides are available [here](https://github.com/EK-Lee/WOMBAT-2017).
* The R packages used for the slides : 

* <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.