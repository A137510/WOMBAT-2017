---
title: "Visualization for Data Mining"
output:
  ioslides_presentation
---

## Conceptual framework

- Visual methods for high-dimensional data are well-developed and understood. Toolbox includes:
    + scatterplot matrices
    + parallel coordinate plots
    + interactive tools like brushing and linking
    + dynamic methods like tours
- These methods can also be used to visualise our models, often leading to some surprise that the fitted models may be different from expected.

## Questions to answer

- What does the model look like? How does the model change when its parameters change? How do the parameters change when the data is changed?
- How well does the model fit the data? How does the shape of the model compare to the shape of the data? Is the model fitting uniformly good, or good in some regions but poor in other regions? Where might the fit be improved?

## Key strategies

- Display the model in data space (m-in-ds), as well as the data in the model space.
- Look at all members of a collection of a model, not just a single one.
- Explore the process of fitting, not just the end result.

## Model in the data space

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(tidyverse)
library(gridExtra)
df <- data.frame(x=runif(100))
df <- df %>% mutate(y=c(5+3*x[-100]+rnorm(99), 15))
df_lm <- lm(y~x, data=df)
df <- df %>% mutate(res = residuals(df_lm))
p1 <- ggplot(df, aes(x=res)) + geom_histogram(binwidth=0.5) + 
  ggtitle("Data in the model space") +
  xlab("Residuals") + theme(aspect.ratio=1)
coefs <- coefficients(df_lm)
p2 <- ggplot(df, aes(x=x, y=y)) + geom_point() +
  geom_abline(intercept=coefs[1], slope=coefs[2]) +
  ggtitle("Model in the data space") + 
  theme(aspect.ratio=1)
grid.arrange(p1, p2, ncol=2)
```

## Model in the data space

```{r echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(MASS)
library(mvtnorm)
g1 <- rmvnorm(n=120, mean=c(1,2), sigma=matrix(c(4,2,2,3), ncol=2)) 
g2 <- rmvnorm(n=80, mean=c(-1,-3), sigma=matrix(c(6,-3,-3,6), ncol=2))
df <- data.frame(x1=c(g1[,1], g2[,1]), x2=c(g1[,2], g2[,2]), g=c(rep("A", 120), rep("B", 80)))
df_lda <- lda(g~., data=df, prior=c(0.5, 0.5))
df <- df %>% mutate(pred = as.vector(predict(df_lda)$x[,1]))
p1 <- ggplot(df, aes(x=pred)) + 
  geom_histogram(breaks=seq(-3, 5, 0.5)) + 
  geom_vline(xintercept=0, colour="red") +
  facet_wrap(~g, ncol=1) +
  ggtitle("Data in the model space")
df_grid <- expand.grid(x1=seq(-10,10,0.2), x2=seq(-11,10,0.2)) 
df_grid$pred <- predict(df_lda, df_grid)$class
p2 <- ggplot(df_grid, aes(x=x1, y=x2, colour=pred)) + 
  geom_point(alpha=0.1) +
  geom_point(data=df, aes(x=x1, y=x2, colour=g), 
             shape=4, size=3, inherit.aes = FALSE) +
  scale_colour_brewer("", palette="Dark2") +
  ggtitle("Model in the data space") + 
  theme_bw() + theme(aspect.ratio=1)
grid.arrange(p1, p2, ncol=2)
```

## Your turn

For each of the two previous displays, write down one thing that you learn differently from the two different ways of looking at the model and data.

```{r echo=FALSE, eval=FALSE}
1. 
D-in-MS: Focus is on residuals, see the outlier clearly
M-in-DS: See that the model fits the middle of the data nicely, outlier may have pulled the line
2. 
D-in-M-S: Predictions are routine, some cases misclassified
M-in-D-S: Variances of the two groups are slightly different
```

## Your turn

This is a typical diagram used to explain how to build a Support Vector Machine. Is it M-in-DS or D-in-MS??

![by cyc on wikipedia](svm.png)

## All members of a collective

- All possible models,  e.g. for a linear regression, we can generate all possible main effects models.
- Same model fit to subsets: neural networks (ensembles of logistic models), random forests (ensembles of trees)

## Linear regression

- Model level: model fit statistics. $m$ observations.
- Model-estimate level: coefficient estimates on various scales. $m \times p$ observations.
- Estimate level: summary of estimates over the models. $p$ observations.
- Model-observation level: residuals and influence measures. $m \times n$ observations.
 Observation level: the original data, plus summaries of residual behavior. $n$ observations.

## All possible models

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, results='hide', fig.show='hide'}
# devtools::install_github("hadley/meifly")
library(meifly)
library(plotly)
library(stringr)

rescale01 <- function(x) (x - min(x, na.rm = TRUE)) / diff(range(x, na.rm = TRUE))

# Fit all linear models to New Haven data --------------------------------------

data(NewHavenResidential, package = "barcode")

y <- scale(NewHavenResidential[,1])
x <- NewHavenResidential[,-c(1,3)]
x$livingArea <- scale(x$livingArea)
x$size <- scale(x$size)
x$bedrms <- scale(x$bedrms)
x$bathrms <- scale(x$bathrms)
models <- fitall(y, x)
# ggobi(models, NewHavenResidential)

# Summary statistics -----------------------------------------------------------

model_sum <- models %>%
  summary() %>%
  tbl_df() %>%
  gather(statistic, val, logL:adjR2) %>%
  group_by(statistic) %>% 
  mutate(stdval = rescale01(val), df = df - 1) %>%
  group_by(statistic, df) %>% mutate(rank = min_rank(desc(val))) %>%
  ungroup()

mnames <- function(m) {
  name <- str_c(names(coefficients(m))[-1], collapse=",")
  return(name)
}
model_r2 <- models %>% 
  plyr::ldply(mnames, .id = "model") %>% 
  dplyr::rename(name=V1) %>%
  right_join(model_sum, by = "model") %>%
  filter(statistic == "R2")

p <- ggplot(model_r2, aes(x=df, y=val, label=name)) +
  geom_point() +
  geom_line(data = filter(model_r2, rank == 1)) +
  #facet_wrap(~statistic, ncol = 5) +
  xlab("Degrees of Freedom") +
  ylab("R2")
```

```{r echo=FALSE}
ggplotly(p)
```

## Coefficients

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, results='hide', fig.show='hide'}
f_coef <- function(x) {
  coef <- coefficients(x)
  df <- matrix(rep(0, 8), ncol=8)
  colnames(df)=c("(Intercept)","livingArea",
         "size","zoneRM","zoneRS","acTypeNo AC", 
         "bedrms","bathrms")
    data.frame(coef=rep(0, 9))
  df[colnames(df) %in% names(coef)] <- coef
  colnames(df)[1] <- "Intercept"
  return(data.frame(df))
}
model_coef <- models %>% 
  map(f_coef) %>%
  map_df(bind_rows) %>%
  select(-Intercept) %>%
  mutate(id = 1:nrow(.)) %>%
  gather(var, val, -id) #%>%
  #group_by(var) %>% 
  #mutate(stdval = rescale(val))
model_coef$var <- factor(model_coef$var, 
                         levels=unique(model_coef$var)) 

```

```{r echo=FALSE}
p <- ggplot(model_coef, aes(x = var, y = val,  
                     key = id, text = id)) + 
  #geom_jitter(width = 0.25, alpha = 0.5) +
    geom_line(aes(group = id), alpha = 0.2) +
    geom_point(alpha = 0.5, size = 0.001) +
    theme_bw() + xlab("") + ylab("Standardised coefficients")
ggplotly(p, tooltip = "text") %>% layout(dragmode = "select")
```

This needs some more work to get brushing going.

## Neural networks

$$P(y \in \textrm{class}_j | x_{1}, ..., x_{p}) = k \cdot \phi ( \alpha + \sum_{h=1}^s w_{hj} \phi (\alpha_h + \sum_{i=1}^p w_{ih} x_i ))$$

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, results='hide', fig.show='hide'}
library(nnet)
library(readr)
w <- read_csv("wiggly.csv")[ ,2:4]
w <- transform(w, class = factor(class, labels = c("A", "B")))
sqr <- theme(aspect.ratio = 1)
nolab <- list(scale_x_continuous(""), scale_y_continuous(""))
col_d <- scale_colour_manual(values = c("#377EB8", "#E41A1C"))
fill_c <- scale_fill_gradient2(low = "#E41A1C", mid = "white", high = "#377EB8",
  midpoint = 0.5)
load("wiggly-multi.rdata")
many <- rbind(many2$coef, many3$coef, many4$coef)
many <- plyr::rename(many, c(size = "nodes"))
many$id <- (many$nodes - 2) * 200 + as.integer(many$i)
source("explore.r")
best <- Filter(function(x) accuracy(x) == 1, many4$all)[[1]]
```

```{r echo=FALSE}
p1 <- ggplot(subset(best$output,  node == 1), aes(x, y)) +
  geom_raster(aes(fill = pred)) +
  geom_point(aes(shape = class), data = w) +
  sqr +
  fill_c +
  nolab
p2 <- ggplot(best$hidden, aes(x, y)) +
  geom_contour(aes(z = pred, group = node), colour = "grey50", size = 2,
    breaks = 0.5) +
  geom_point(aes(colour = class, shape = class), data = w) +
  sqr + col_d + nolab
grid.arrange(p1, p2, ncol=2)
```

## Hidden nodes

```{r echo=FALSE}
hidden_fill <- scale_fill_gradient2(low = "#AF8DC3", mid = "#F7F7F7",
  high = "#7FBF7B", midpoint = 0.5, limits = c(0, 1))

ggplot(best$hidden, aes(x, y)) +
  geom_raster(aes(fill = pred)) +
  geom_point(aes(shape = class), data = w) +
  sqr +
  facet_grid(. ~ node) +
  hidden_fill +
  nolab
```

## Your turn

1. In the formula for the model, what is $s$, $p$.
3. Sketch the neural network diagram that corresponds to the best model.

## Explore the process of fitting

<iframe src="https://player.vimeo.com/video/767832" width="640" height="483" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

## Model comparison

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, results='hide'}
qual <- unique(many[, c("value", "accuracy", "nodes", "id")])
ggplot(data = qual, aes(x=accuracy, y=value)) + 
  geom_point() +
  xlab("Accuracy") +
  ylab("Value of fitting criterion") + 
  facet_wrap(~ nodes)
```

## Boundaries for each model

```{r echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, results='hide'}
all <- mapply(function(nn, i, j) {
  df <- subset(nn$output, node==1)
  df$id <- i
  df$nodes <- j
  df
}, c(many2$all,many3$all,many4$all), 1:600, 
  rep(2:4, each=200), SIMPLIFY = FALSE)
all <- do.call("rbind", all)

contours <- plyr::ddply(all, plyr::.(nodes, id), 
              function(df) contour(df$x, df$y, df$pred))
contours <- subset(contours, piece == 1)
contours <- merge(contours, qual)
contours$acc <- cut(contours$accuracy, c(0, 0.915, 0.925, 0.98, 1),
  labels = c("Sub-linear", "Linear", "Super-linear", "Excellent"))

ggplot(contours, aes(x, y)) + sqr +
  geom_path(aes(group = id), 
            colour = scales::alpha("grey50", 0.5)) +
  facet_grid(nodes ~ acc) +
  nolab
```

## Random forest

- Tree-level: each tree has its own training/test samples
- Variable-level: each variable is ranked by the drop in model performance for each tree
- Observation-level: for each observation we have the distribution of predictions across all trees

We will come back to this later, because the area is very rich.


## My notes

- show how to extract SVs
- forest components

## Outline

* The Toolbox: Grammar of graphics, multivariate data plots, dimension reduction, interactive graphics
* Supervised Classification: Checking assumptions, examining misclassifications, variable importance, visualising boundaries, exploring ensembles
* Trees: Exploring splits, oblique trees
* Forests: Variable importance, confusion table, vote matrix, 
proximity matrix, oblique forest shiny app
* Support vector machines: support vectors, boundary
* Neural networks: Components
* High-dimension, small sample size

## Toolbox

* Grammar of graphics
* Multivariate data plots 
* Dimension reduction 
* Interactive graphics

## Grammar of graphics

* 

## Multivariate plots

* Parallel coordinate plot
* Tours

## Dimension reduction

* Principal componenent analysis
* Projection pursuit

## Supervised Classification

* Checking assumptions
* Examining misclassifications
* Variable importance
* Visualising boundaries
* Exploring ensembles

## Checking assumptions

* LDA: equal variance-covariances
* 

## Examining misclassifications

Linking confusion table with data plots

## Variable importance

Ordering par coords by importance

## Visualising boundaries

Predicting grid and viewing with tour

## Exploring ensembles

* Diagnostics
* Pruning

## Trees

## Sources

- Wickham, Cook, Hofmann (2015) Visualizing statistical models: Removing the blindfold, Statistical Analysis and Data Mining, (http://dx.doi.org/10.1002/sam.11271)
- Cook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis: With Examples Using R and GGobi (http://www.ggobi.org/book/)

